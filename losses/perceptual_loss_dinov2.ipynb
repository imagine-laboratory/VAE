{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7af48e9",
   "metadata": {},
   "source": [
    "# Dino for perceptual loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6d0b56",
   "metadata": {},
   "source": [
    "Download the backbones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36278cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/rtxmsi1/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/home/rtxmsi1/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/home/rtxmsi1/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/home/rtxmsi1/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dinov2_vits14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')\n",
    "#dinov2_vitb14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')\n",
    "#dinov2_vitl14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14')\n",
    "#dinov2_vitg14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a0613e",
   "metadata": {},
   "source": [
    "Read the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d097351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 126, 126])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import transforms as pth_transforms\n",
    "from PIL import Image\n",
    "\n",
    "img = Image.open(\"/home/rtxmsi1/Documents/DINO/src/Vd-Orig.png\")\n",
    "img = img.convert('RGB')\n",
    "transform = pth_transforms.Compose([\n",
    "        pth_transforms.Resize(126),\n",
    "        pth_transforms.ToTensor(),\n",
    "        pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ])\n",
    "\n",
    "img = transform(img)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4f6aad",
   "metadata": {},
   "source": [
    "### Using CLS token\n",
    "\n",
    "Pros: Designed as a global summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca2f963b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3840])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/facebookresearch/dino/blob/main/eval_linear.py#L153\n",
    "avgpool = False\n",
    "n = 10\n",
    "\n",
    "with torch.no_grad():\n",
    "    if \"vit\":\n",
    "        intermediate_output = dinov2_vits14.get_intermediate_layers(img.unsqueeze(0), n)\n",
    "        # Get the CLS token for each intermediate output\n",
    "        output = torch.cat([x[:, 0] for x in intermediate_output], dim=-1)\n",
    "        if avgpool:\n",
    "            output = torch.cat((output.unsqueeze(-1), torch.mean(intermediate_output[-1][:, 1:], dim=1).unsqueeze(-1)), dim=-1)\n",
    "            output = output.reshape(output.shape[0], -1)\n",
    "    else:\n",
    "        output = dinov2_vits14(img)\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "feff86a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3840])\n",
      "tensor(-9.9938) tensor(9.9907)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = (torch.rand(1, 3840) * 20) - 10  # Scale [0,1) → [0,20) → [-10,10)\n",
    "print(x.shape)  # torch.Size([1, 3840])\n",
    "print(x.min(), x.max())  # Should be near -10 and 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2466f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(193096.2031)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "mse = F.mse_loss(output, x, reduction=\"sum\")\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36094f48",
   "metadata": {},
   "source": [
    "### Mean‑pooled tokens\n",
    "\n",
    "What: average over all tokens (patch embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b34dc107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate_output[1].mean(dim=1).shape\n",
    "\n",
    "mse = F.mse_loss(intermediate_output[1].mean(dim=1), intermediate_output[1].mean(dim=1), reduction=\"sum\")\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d257d0",
   "metadata": {},
   "source": [
    "### Full token map (spatial loss)\n",
    "Pros: Preserves spatial correspondence, analogous to VGG “feature map” loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c90ba8d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate_output[1]\n",
    "\n",
    "mse = F.mse_loss(intermediate_output[1], intermediate_output[1], reduction=\"sum\")\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7411e82",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94e973ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def dino_perceptual_loss(\n",
    "    x_real,\n",
    "    x_recon,\n",
    "    dino_model,\n",
    "    layer_ids=[11],\n",
    "    mode='cls',         # 'cls', 'mean', or 'tokens'\n",
    "    reduction='mean'    # or 'none'\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute perceptual loss between x_real and x_recon using DINO ViT features.\n",
    "\n",
    "    Args:\n",
    "        x_real (Tensor): Original image batch [B, 3, H, W]\n",
    "        x_recon (Tensor): Reconstructed image batch [B, 3, H, W]\n",
    "        dino_model (nn.Module): DINO ViT model with get_intermediate_layers\n",
    "        layer_ids (list[int]): Layer indices to use for perceptual comparison\n",
    "        mode (str): 'cls' | 'mean' | 'tokens'\n",
    "        reduction (str): 'mean' | 'sum' | 'none'\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Scalar loss (or per-sample if reduction='none')\n",
    "    \"\"\"\n",
    "    # Get intermediate layers\n",
    "    with torch.no_grad():\n",
    "        feats_real = dino_model.get_intermediate_layers(x_real, n=len(dino_model.blocks)+1)\n",
    "        feats_recon = dino_model.get_intermediate_layers(x_recon, n=len(dino_model.blocks)+1)\n",
    "\n",
    "    loss = 0.0\n",
    "\n",
    "    for layer in layer_ids:\n",
    "        f_real = feats_real[layer]  # [B, T, D]\n",
    "        f_recon = feats_recon[layer]\n",
    "\n",
    "        if mode == 'cls':\n",
    "            v_real = f_real[:, 0]    # CLS token\n",
    "            v_recon = f_recon[:, 0]\n",
    "\n",
    "        elif mode == 'mean':\n",
    "            v_real = f_real.mean(dim=1)\n",
    "            v_recon = f_recon.mean(dim=1)\n",
    "\n",
    "        elif mode == 'tokens':\n",
    "            v_real = f_real\n",
    "            v_recon = f_recon\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown mode: {mode}\")\n",
    "\n",
    "        loss += F.mse_loss(v_real, v_recon, reduction=reduction)\n",
    "\n",
    "    return loss / len(layer_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c76efb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360eef27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
